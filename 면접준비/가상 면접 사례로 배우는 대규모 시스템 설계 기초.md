# 5장 안정 해시 설계
- 수평적 규모 확장성을 달성하기 위해서는 요청 또는 데이터를 서버에 균등하게 나누는 것이 중요
- 이때 안정 해시를 보편적으로 사용

## 해시 키 재배치 문제

- N개의 캐시 서버가 있고 부하를 균등하게 나누기 위해 아래 해시 삼수를 사용
    + ServerIndex = hash(key) % N(서버 대수)
    + key0 키의 경우 해시가 18358617일때 % 4를 하면 1의 ServerIndex가 나온다
- 위 경우에서 서버의 풀에 변화가 생기면 문제가 발생
- 1번 서버로 가던 요청이 1번 서버의 장애로 서버 풀이 변경되면 해시 재배치가 일어나는데 이때 대규모 캐시 미스가 발생

## 안정 해시
- 해시 테이블 크기가 조정될 때 평균적으로 오직 k/n개의 키만 재배치하는 해시 기술
    + k = 키의 개수
    + n = 슬롯의 개수
    + 전통적인 해시 테이블은 슬롯의 수가 변경되면 대부분 키를 재배치 함

### 해시 공간과 해시 링
- SHA-1로 예를 들때 해시 공간 범위는 0부터 2^160 - 1까지라 알려져 있음
- 따라서 x0은 0, xn은 2^160 - 1이다
- 이 해시 공간을 링으로 만들면 x0과 xn이 맞닿은 모양이 된다

### 해시 서버
- 이때 해시 함수를 사용해서 서버의 정보로 해시 링의 어떤 위치에 대응 시킬 수 있음
    + 해시 슬롯에 서버 정보로 해시 함수를 돌려 저장한다는 뜻

### 해시 키
- 이때 키에 해시 함수를 돌려 나머지 연산을 하지 않고 해시 키도 슬롯에 배치를 시킴

### 서버 조회
- 이때 저장된 해시 키는 시계 방향으로 링을 탐색하면서 만나는 첫 번째 서버를 조회함

### 서버 추가 및 제거
- 위 셜명한 내용에 따르면 서버를 추가 또는 제거하면 키 가운데 일부분만 재배치하면 됨
- 나머지 키에는 영향이 없음

## 기본 구현법의 두 가지 문제
- 서버가 추가되거나 삭제되는 상황을 감안하면 파티션의 크기를 균등하게 유지하는게 불가능
- 키의 균등 분포를 달성하기가 어려움
- 해당 문제를 해결하기 위해 제안된 기법이 가상 노드 또는 복제 기법

### 가상 노드
- 실제 노드 또는 서버를 가리키는 노드로서 하나의 서버는 링 위에 여러 개의 가상 노드를 가질 수 있다
- 가상 노드 개수를 늘리면 키의 분포는 점점 균등해 짐

# 6장 키-값 저장소 설계
- 키-값 저장소는 키-값 데이터베이스라고도 불리는 비 관계형 데이터베이스다
- 이 저장소에 저장되는 값은 고유 식별자를 키로 가져야 함
- 키와 값 사이의 연결 관계를 "키-값"쌍이라고 지칭
    + 키는 유일해야만 함
    + 값은 해당 키를 통해서만 접근 가능
- 키는 일반 텍스트일 수도 있고 해시 값 등이 될 수 있음
- 갑승ㄴ 문자열이나, 리스트, 객체 등일 수 있다
- 키-값 저장소로 널리 알려진것은 AWS 다이나모, memcached, Redis 등이 있다

## 문제 이해 및 설계 범위 확정
- 완벽한 설계란 없음
- 읽기, 쓰기, 메모리 사용량 사이에서 균형을 찾아 데이터 일관성과 가용성 사이에서 타협적 결정을 내리면 됨

## 단일 서버 키-값 저장소
- 가장 직관적인 방법은 키-값 쌍 전부를 메모리에 해시 테이블로 저장
    + 빠른 속도를 보장하긴 하지만 모든 데이터를 메모리에 두는 것이 불가능할 수 있음
    + 해결책으로는 데이터 압축, 자주 사용되는 데이터만 메모리에 적재 등이 있다

## 분산 키-값 저장소
- 분산 시스템을 설계할 때는 CAP 정리를 이해해야 함

```
CAP 정리
- 데이터 일관성(consistency), 가용성(availability), 파티션 감내(partition tolerance)라는 세 가지 요구사항을 동시에 만족하는 분산 시스템을 설계하는 것은 불가능하다는 정리
```
- 데이터 일관성: 분산 시스템에 접속하는 모든 클라이언트는 어떤 노드에 접속하든 언제나 같은 데이터를 받아야 한다
- 가용성: 분산 시스템에 접속하는 클라이언트는 일부 노드에 장애가 발생하더라도 항상 응답을 받을 수 있어야 한다
- 파티션 감내: 파티션은 두 노드 사이에 통신 장애가 발생하였음을 의미. 파티션 감내는 네트워크에 파티션이 생기더라도 시스템은 계속 동작 해야함
- 위 요구사항 중 어떤 두 가지를 만족하면 하나는 반드시 희생해야 함
    + CP 시스템: 일관성과 파티션 감내를 지원. 가용성 희생
    + AP 시스템: 가용성과 파티션 감내를 지원. 데이터 일관성 희생
    + CA 시스템: 일관성과 가용성 지원. 파티션 감내 희생. 하지만 통상 네트워크 장애는 피할 수 없는 일로 여겨지므로, 분산 시스템은 반드시 파티션 문제를 감내할 수 있도록 설계
- 은행 같은 곳은 데이터 일관성이 중요하기 때문에 CP 시스템을 사용할것이고 뉴스 같은 컨텐츠를 제공하는 곳은 가용성이 중요하기 때문에 AP 시스템을 사용

## 시스템 컴포넌트
- 키-값 저장소 구현에 사용될 핵심 컴포넌트들 및 기술

### 데이터 파티션
- 대규모 시스템의 경우 데이터를 나눠서 저장해야하는데 가장 단순한 방법은 작은 파티션들로 분할한 다음 여러 대 서버에 저장하는 것
- 파티션 단위로 나눌 때는 아래 두 가지 문제가 중요
    + 데이터를 여러 서버에 고르게 분산 가능한가
    + 노드가 추가되거나 삭제될 때 데이터의 이동을 최소화할 수 있는가
- 5장에서 다룬 안정 해시는 위 문제를 푸는데 적합
    + 규모 확장 자동화: 시스템 부하에 따라서 서버가 자동으로 추가되거나 삭제되도록 가능
    + 다양성: 각 서버의 용량에 맞게 가상 노드 수를 조정 할 수 있음

### 데이터 다중화
- 높은 가용성과 안정성을 확보하기 위해서 데이터를 N개 서버에 비동기적으로 다중화 할 필요가 있음
- N개 서버는 해시링 위에서 시계 방향으로 순회하며 만나는 N개의 서버에 데이터 사본을 저장
    + 가상 노드를 사용하면 선택한 N개의 노드가 대응될 실제 물리 서버의 개수가 N보다 작아질 수 있음
    + 그래서 노드 선택할 때 중복된 물리 서버를 선택하지 않도록 처리

### 데이터 일관성
- 여러 노드에 다중화된 데이터는 적절히 동기화가 되어야 함
- 정족수 합의 프로토콜을 사용하면 읽기/쓰기 연산 모두에 일관성을 보장할 수 있음
    + N = 사본 개수
    + W = 쓰기 연산에 대한 정족수. 쓰기 연산이 성공한 것으로 간주되려면 적어도 W개의 서버로부터 쓰기 연산이 성공했다는 응답을 받아야 함
    + R = 읽기 연산에 대한 정족수. 읽기 연산이 성공한 것으로 간주되려면 적어도 R개의 서버로부터 응답을 받아야 함
    + R=1, W=N: 빠른 읽기 연산에 최적화
    + W=1, R=N: 빠른 쓰기 연산에 최적화
    + W+R > N: 강한 일관성이 보장됨
    + W+R <= N: 강한 일관성이 보장되지 않음

### 일관성 모델
- 일관성 모델은 데이터 일관성의 수준을 결정
- 강한 일관성: 모든 읽기 연산은 가장 최근에 갱신된 결과를 반환
- 약한 일관성: 가장 최근에 갱신된 결과를 반환하지 못할 수 있음
- 최종 일관성: 갱신 결과가 결국에는 모든 사본에 반영되는 모델
    + 데이터에 버전 정보를 넣어 클라이언트에서 일관성이 깨진 데이터를 읽지 않도록 함

### 비 일관성 해소 기법: 데이터 버저닝
- 데이터를 다중화하면 사본 간 일관성이 깨질 가능성이 높음
- 버저닝과 벡터 시계로 문제를 해소할 수 있음
    + 버저닝: 데이터를 변경할 때마다 해당 데이터의 새로운 버전을 생성. 각 버전의 데이터는 변경 불가능
    + 벡터 시계: 데이터 버저닝 과정에서 버전간 충돌이 발생했을 때 사용되는 기술로 [서버, 버전]의 순서쌍을 데이터에 매단 것

### 장애 처리
- 대규모 시스템에서 장애는 아주 흔한 일이기 때문에 장애를 어떻게 처리할 것인지는 굉장히 중요한 문제

### 장애 감지
- 분산 시스템에서는 노드 사이에 멀티캐스팅 채널을 구축해서 서로 장애를 감지할 수 있음
    + 서버가 많을 때는 비효율적인 방법
- gossip protocol 같은 분산형 장애 감지 솔루션이 효율적입
    + 각 노드는 멤버십 목록을 유지. 멤버심 목록은 각 멤버 ID와 해당 멤버의 heartbeat counter 쌍의 목록임
    + 각 노드는 주기적으로 자신의 heartbeat counter를 증가 시킴
    + 각 노드는 무작위로 선정된 노드들에게 주기적으로 자기의 heartbeat counter 목록을 보냄
    + heartbeat counter 목록을 받은 노드는 멤버십 목록을 최신화
    + 어떤 멤버의 heartbeat counter 값이 지정된 시간 동안 갱신되지 않으면 해당 멤버는 장애 상태인 것으로 간주

### 일시적 장애 처리
- 장애를 감지한 시스템은 가용성을 보장하기 위해 필요한 조치를 해야 함
- 엄격한 정족수 접근법을 쓴다면 읽기와 쓰기 연산을 금지
- 느슨한 정족수 접근법을 쓴다면 쓰기 연산을 수행할 W개의 서버와 읽기 연산을 수행할 R개의 건강한 서버버를 해시 링에서 고름
- 네트워크나 서버 문제로 장애 상태인 서버로 가는 요청은 다른 서버로 우회
    + 해당 시간동안 발생한 변경 사항은 복구되었을 때 일괄 반영하여 데이터 일관성 보존
    + 이를 위해 연산을 처리한 서버는 단서를 남겨둠
    + 이런 장애 처리 방안을 단서 후 임시 위탁 기법이라 부름

### 영구 장애 처리
- hinted handoff 기법은 일시적 장애를 처리하기 위한 방법
- 영구적인 노드 장애 처리에는 반-엔트로피(anti_entropy) 프로토콜을 구현하여 사본들을 동기화
    + 사본들을 비교하여 최신 버전으로 갱신하는 과정
    + 사본 간의 일관성이 망가진 상태를 탐지하고 전송 데이터의 양을 줄이기 위해서 Merkle 트리를 사용

```
머클 트리
- 해시 트리라고도 불리는 머클 트리는 각 노드에 그 자식 노드들에 보관된 값의 해시 또는 자식 노드들의 레이블로부터 계산된 해시 값을 레이블로 붙여두는 트리이다. 해시 트리를 사용하면 대규모 자료 구조의 내용을 효과적이면서도 보안상 안전한 방법으로 검증할 수 있다
```
1. 키 공간을 묶어서 버킷으로 나눔
2. 버킷별로 해시값을 계산한 후, 해당 버킷의 합을 다시 해시하여 해당 해시 값을 레이블로 갖는 노드를 만듬
3. 상향식으로 자식 노드의 레이블로부터 새로운 해시 값을 계산하여 이진 트리를 구성
4. 이렇게 완성된 두 머클 트리를 루트 노드부터 비교해서 틀린 부분만 찾아나감

### 데이터 센터 장애 처리
- 데이터 센터는 다양한 이유로 장애가 발생할 수 있다.
- 장애에 대응할 수 있는 시스템을 만들려면 여러 데이터 센터에 다중화하는 것이 중요

# 7장 - 분산 시스템을 위한 유일 ID 생성기 설계
- 분산 환경에서는 Auto Increment 속성을 사용하는 관계형 데이터베이스 기본키를 사용할 수 없음
    + 데이터베이스 서버 한 대로는 요청을 감당할 수 없고 여러 데이터베이스 서버를 쓰는 경우에는 지연 시간을 낮추기가 힘듬

## 1단계 - 문제 이해 및 설계 범위 확정
- ID는 어떤 특성을 갖나?
- 새로운 레코드에 붙일 ID는 항상 1만큼 큰 값이어야 하나?
- ID는 숫자로만 구성되나?
- 시스템 규모는 어느정도 인가?

## 2단계 - 개략적 설계안 제시 및 동의 구하기
- 유일성이 보장되는 ID를 만드는 방법은 여러가지
    + 다중 마스터 복제
    + UUID
    + 티켓 서버
    + 트위터 스노플레이크 접근법

### 다중 마스터 복제
- 해당 접근법은 데이터베이스의 auto_increment 기능을 활용하는 것. 다만 다음 ID를 구할 때 K만큼 증가 시킴
    + K는 현재 사용 중인 데이터베이스 서버의 수
- 규모 확장성 문제를 해결할 수는 있지만 중대한 단점이 있음
    + 여러 데이터 센터에 걸쳐 규모를 늘리기 어려움
    + 유일성은 보장되겠지만 값이 시간 흐름에 맞추어 커지도록 보장할 수는 없음
    + 서버를 추가하거나 삭제할 때도 잘 동작하도록 만들기 어려움

### UUID
- 컴퓨터 시스템에 저장되는 정보를 유일하게 식별하기 위한 128비트짜리 수
- 각각의 웹 서버가 별도의 ID 생성기를 사용해 독립적으로 ID를 만들 수 있음
- 장점
    + UUID를 만드는 것은 단순함. 동기화 이슈도 없음
    + 각 서버가 ID를 알아서 만드는 구조이므로 규모 확장도 쉬움
- 단점
    + ID가 128비트로 김
    + 시간순으로 정렬할 수 없음
    + 숫자 아닌 값이 포함될 수 있음

### 티켓 서버
- auto_increment 기능을 갖춘 데이터베이스 서버를 중앙 집중형으로 사용
- 장점
    + 유일성 보장이 되는 숫자로만 구성된 ID를 쉽게 만듬
    + 구현하기 쉽고, 중소 규모 애플리케이션에 적합
- 단점
    + 티켓 서버가 SPOF(Single-Point-of-Failure)가 됨

### 트위터 스노플레이크 접근법
+ 생성해야 하는 ID의 구조를 여러 절로 분할해 생성
    + 사인 비트: 1비트 할당. 쓰임새가 없지만 나중을 위해 유보
    + 타임스탬프: 41비트 할당. epoch(기원 시각)) 이후로 몇 밀리초가 경과했는지를 나타내는 값
    + 데이터센터 ID: 5비트 할당
    + 서버 ID: 5비트 할당
    + 일련번호: 12비트 할당. 각 서버에서 ID를 생성할 때마다 해당 일련번호를 1만큼 증가. 1밀리초가 지날때마다 0으로 초기화

# 8장 - URL 단축기 설계

## 1단계 - 문제 이해 및 설계 범위 확정
- 항상 성공적으로 설계하려면 질문을 통해 모호함을 줄이고 요구사항을 알아야 함
    + 쓰기 연산: 매일 1억 개의 단축 URL 생성
    + 초당 쓰기 연산: 1160
    + 읽기 연산: 읽기 연산과 쓰기 연산 비율을 10:1로 하면 초당 11,600회
    + 10년간 운영한다고 하면 3650억개의 레코드를 보관
    + 축약 전 URL의 평균 길이는 100정도
    + 10년동안 필요한 저장 용량은 3650억 X 100바이트 = 36.5TB

## 2단계 - 개략적 설계안 제시 및 동의 구하기

### API 엔드포인트
- URL 단축기에는 기본적으로 두 개의 엔드포인트가 필요
    + URL을 단축을 위한 엔드포인트: 클라이언트가 단축하고자 하는 URL을 담아서 POST 요청을 보냄
    + URL 리디렉션용 엔드포인트: 단축 URL에 대해서 원래 URL로 보내주기 위한 용도

### URL 리디렉션
- 단축 URL을 받은 서버는 해당 URL을 원래 URL로 바꾸어서 301응답의 Location 헤더에 넣어 반환
    + 301 Permanently Moved: 해당 URL에 대한 HTTP 요청의 처리 책임이 영구적으로 Location 헤더에 반환된 URL로 이전되었다는 뜻
    + 302 Found: 주어진 URL로의 요청이 일시적으로 Location 헤더가 지정하는 URL에 의해 처리되어야 한다는 응답
- 서버 부하를 줄일때는 301이 좋고 트래픽 분석이 중요할 때는 302를 쓰는 쪽이 좀더 유리
- URL 리디렉션을 구현하는 가장 직관적인 방법은 해시 테이블을 사용

### URL 단축
- 중요한 것은 긴 URL을 해당 해시 값으로 대응시킬 해시 함수 fx를 찾은 것
- 해시 함수는 아래 요구사항을 만족
    + 입력으로 주어지는 긴 URL이 다른 값이면 해시 값도 달라야 한다
    + 계산된 해시 값은 원래 입력으로 주어졌던 긴 URL로 복원될 수 있어야 함

## 3단계 - 상세 설계

### 데이터 모델
- 개략적 설계에서는 모든 것을 해시 테이블에 두었지만 실제 시스템에 적용하기는 곤란
- 더 좋은 방법으로는 관계형 데이터베이스에 쌍을 저장
    + id, shortURL, longURL의 세 개 칼럼을 가짐

### 해시 함수
- 원래 URL을 단축 URL로 변환하는 데 사용

#### 해시 값 길이
- hashValue는 [0-9, a-z, A-Z]의 문자들로 구성
- 사용할 수 있는 문자의 개수는 62개. 10 + 26 + 26 = 62
- hashValue 길이를 정하기 위해서는 62^n >= 3650억인 n의 최솟값을 찾아야 함
    + n=7이면 3.5조개의 URL을 만들 수 있음

#### 해시 후 충돌 해소
- CRC32, MD5, SHA-1 같은 해시 함수를 이용해 단축 후 앞에 7자를 사용하면 충돌 확률이 높아 짐
- 첫번째 방법은 충돌이 해소될 때까지 사전에 정한 문자열을 해시값에 덧붙임
    + 충돌은 해소할 수 있지만 데이터베이스 질의를 해야하므로 오버헤드가 큼

#### base-62 변환
- 진법 변환은 URL 단축기를 구현할 때 흔히 사용되는 접근법
- 해당 기법은 수의 표현 방식이 다른 두 시스템이 같은 수를 공유하여야 하는 경우에 유용
- hashValue의 문자 개수가 62개이기 때문에 base-62 사용
- 62진법은 0은 0, 9는 9, a는 11... 61은Z로 대응시켜 표현
- ID의 유일성이 보장되진 않음

### URL 단축기 상세 설계
1. 입력으로 긴 URL을 받음
2. 데이터베이스에 해당 URL이 있는지 검사
3. 있다면 단축 URL을 가져와 반환
4. 없다면 DB의 기본기로 사용된 유일한 ID를 생성
5. 62진법 변환을 적용. ID를 단축 URL로 생성
6. ID, 단축 URL, 원래 URL로 새 데이터베이스 레코드를 만든 후 단축 URL을 클라이언트에 전달

# 9장 - 웹 크롤러 설계
- 웹 크롤러: 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적
- 크롤러가 이용되는 방법
    + 검색 엔진 인덱싱
    + 웹 아카이빙: 장기보관하기 위해 웹에서 정보를 모으는 절차
    + 웹 마이닝
    + 웹 모니터링: 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링 할 수 있음

## 1단계 - 문제 이해 및 설계 범위 확정
- 웹 크롤러의 기본 알고리즘은 간단함
    1. URL 집합이 주어지면, 해당 URL들이 가리키는 모든 웹 페이지를 다운로드
    2. 다운받은 웹 페이지에서 URL을  추출
    3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 1번부터 반복
- 엄청난 규모 확장성을 갖는 웹 크롤러를 설계하는 것은 엄청나게 어려운 작업. 설계 하기 전 요구사항을 알아내고 설계 범위를 좁히는게 중요
    + 크롤러의 주된 용도
    + 수집해야하는 웹 페이지 수
    + 새로 만들어졌거나 수정된 웹 페이지도 포함
    + 저장 여부
    + 중복 콘텐츠 처리
    + 규모 확장성
    + 안정성: 잘못된 링크들에 대해서 잘 대응할 수 있어야 됨
    + 예절: 수집 대상 웹 사이드에 짧은 시간 동안 너무 많이 요청하면 안됨
    + 확장성

### 개략적 규모 추정
- 매달 10억 개의 웹 페이지를 다운로드해야 함
- QPS=10억 / 30일 / 24시간 / 3600초 = 약 400페이지/초
- 최대 QPS = 2 X QPS = 800
- 웹 페이지의 크기 평균은 500k라고 가정
- 10억 페이지 x 500k = 500TB/월
- 1개월치 데이터를 보관하는 데는 500TB. 5년간 보관한다고 가정하면 500TB X 12개월 X 5년 = 30PB의 저장용량 필요

## 2단계 - 개략적 설계안 제시 및 동의 구하기
1. 시작 URL 집합 -> 2. 미수집 URL 저장소 -> 3. HTML 다운로더 -> 4. 컨텐츠 파서 -> 5. 중복 컨텐츠?
-> 6. URL 추출기 -> 7. URL 필터 -> 8. 이미 방문한 URL? -> <8-예>: 2 | <8-아니오>: 10. URl 저장소

### 시작 URL 집합
- 크롤러가 크롤링을 시작하는 출발점
- 특정 대학 웹사이트로부터 찾아 나갈 수 있는 모든 웹 페이지를 크롤링하는 가장 직관적인 방법은 해당 대학의 도메인 이름이 붙은 모든 페이지의 URL을 시작 URL로 사용

### 미수집 URL 저장소
- 웹 크롤러는 크롤링 상태를 다운로드할 URL과 다운로드된 URL 두 가지로 나눠 관리
- 다운로드할 URL을 관리하는 컴포넌트를 미수집 URL 저장소라고 부른다

### HTML 다운로더
- 인터넷에서 웹 페이지를 다운로드하는 컴포넌트

### 도메인 이름 변환기
- 웹 페이지를 다운받으려면 URL을 IP 주소로 변환하는 절차가 필요
- HTML 다운로더는 도메인 이름 변환기를 사용하여 URl에 대응되는 IP 주소를 알아낸다

### 콘텐츠 파서
- 웹 페이지를 다운로드하면 파싱과 검증 절차를 거쳐야 함
- 이상한 웹 페이지는 문제를 일으킬 수 있고 저장 공강만 낭비하게 됨

### 중복 콘텐츠인가?
- 연구 결과에 따르면, 29% 가량의 웹 페이지 콘텐츠는 중복임
- 위 문제를 해결하기 위해 자료 구조를 도입해서 데이터 중복을 줄이고 데이터 처리에 소요되는 시간을 줄임

### 콘텐츠 저장소
- HTML 문서를 보관하는 시스템
- 저장소를 구현할때는 저장할 데이터의 유형, 크기, 저장소 접근 빈도, 데이터의 유효 기간 등을 종합적으로 고려

### URL 추출기
- HTML 페이지를 파싱하여 링크들을 골라내는 역할을 함
    + HTML 페이지 내의 상대 경로를 전부 도메인을 붙여 절대 경로로 변환

### URL 필터
- 특정한 콘텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL, 접근 제외 목록에 포함된 URL 등을 크롤링 대상에서 배제하는 역할을 함

### 이미 방문한 URL?
- 이 단계를 위해서 보간된 URL을 추적할 수 있도록 하는 자료 구조를 사용
- 해당 기능으로 같은 URL을 여러번 처리하는 일을 방지할 수 있고 시스템이 무한 루프에 빠지는 일을 방지할 수 있음
- 해당 자료 구조는 블룸 필터를 이용

```
블룸 필터
원소가 집합에 속하는지 여부를 검사하는데 사용되는 확률적 자료 구조.
어떤 원소가 집합에 속한다고 판단된 경우 실제로는 원소가 속하지 않는 긍정 오류가 발생하는 것이 가능하지만, 반대로 원소가 집합에 속하지 않는 것으로 판단되었는데 실제로는 원소가 집합에 속하는 부정 오류는 절대 발생하지 않음.
집합에 원소를 추가하는 것은 가능하나, 집합에서 원소를 삭제하는 것은 불가능
```

### URL 저장소
- 이미 방문한 URL을 보관하는 저장소

## 3단계 - 상세 설계
- 아래는 가장 중요한 컴포넌트 목록
    + DFS vs BFS
    + 미수집 URL 저장소
    + HTML 다운로더
    + 안정성 확보 전략
    + 확장성 확보 전략
    + 문제 있는 콘텐츠 감지 및 회피 전략

### DFS를 쓸 것인가, BFS를 쓸 것인가
- 웹은 유향 그래프(directed graph) 같다
    + 페이지가 노드이고 하이퍼링크가 에지라고 보면 됨
- 크롤링 프로세스는 유향 그래프를 에지를 따라 탐색하는 과정
- DFS는 좋은 선택이 아닐 가능성이 높음
    + 그래프 크기가 클 경우 어느 정도로 깊숙이 가게 될지 가늠이 어려움
- 그래서 웹 크롤러는 보통 BFS를 사용
    + FIFO 방식으로 탐색할 URL을 넣고 한쪽에서 꺼내서 처리한다
- 하지만 BFS에는 두 가지 문제 점이 있음
    + 한 페이지에서 나오는 링크는 대부분 같은 서버로 되돌아 감. 그럼 크롤러는 같은 호스트에 속한 많은 링크를 받게되고 다운을 병렬로 처리하게 되면 해당 서버는 수많은 요청으로 과부하에 걸림. 이 같은 크롤러를 보통 예의 없는(impolite) 크롤러라 함
    + 표준적 BFS 알고리즘은 URl 간에 우선순위를 두지 않음

### 미수집 URL 저장소
- 해당 저장소를 활용하면 위 문제를 쉽게 해결할 수 있음
- 해당 저장소를 잘 구현하면 예의를 갖춘 크롤러, URL 사이의 우선순위와 신선도를 구별하는 크롤러를 구현할 수 있음

### HTML 다운로더
- HTTP 프로토콜을 통해 웹 페이지를 내려 받음

#### Robots.txt
- 로봇 제외 프로토콜이라고도 부르는 Robots.txt는 웹사이트가 크롤러와 소통하는 표준적 방법임
    + 해당 파일에는 크롤러가 수집해도 되는 페이지 목록이 들어 있음

#### 성능 최적화
1. 분산 크롤링 : 성능을 높이기 위해 크롤링 작업을 여러 서버에 분산하는 방법
2. 도메인 이름 변환 결과 캐시: 도메인 이름 변환기는 크롤러 성능 병목 중 하나. DNS 요청을 보내고 결과를 받는 동기적 특성 때문. 크롤러 스레드 가운데 하나가 이 작업을 하고 있으면 다른 스레드의 DNS 요청은 전부 블록됨. 그래서 결과를 캐시하고 크론 잡 등을 돌려 주기적으로 갱신하도록 해 놓으면 성능을 효과적으로 높일 수 있다
3. 지역성: 크롤링 작업을 수행하는 서버를 지역별로 분산. 크롤링 서버가 대상 서버와 지역적으로 가짜우면 다운로드 시가이 줄어들것
4. 짧은 타임아웃: 어떤 웹 서버는 응답이 느리거나 아예 응답하지 않음. 그래서 최대 얼마나 기다릴지를 미리 정해 둠

### 안정성
- 안정성도 다운로더 설계 시 중요하게 고려해야 함
- 안정성을 향상시키기 위한 접근법 가운데 중요한 몇 가지
    + 안정 해시
    + 크롤링 상태 및 수집 데이터 저장: 장애가 발생한 경우에도 쉽게 복구할 수 있도록 크롤링 상태와 수집된 데이터를 지속적 저장장치에 기록해 두는것이 바람직
    + 예외 처리: 대규모 시스템에서 에러는 흔하게 일어남. 예외가 발생해도 전체 시스템이 중단되는 일 없이 작업을 우아하게 이어나갈 수 있어야 한다
    + 데이터 검증: 시스템 오류를 방지하기 위한 중요한 수단

### 확장성
- 시스템은 계속 진화하기 때문에, 새로운 형태의 콘텐츠를 쉽게 지원할 수 있도록 신경 써야 함

### 문제 있는 콘텐츠 감지 및 회피
1. 중복 콘텐츠
    - 웹 콘텐츠의 30% 가량은 중복임. 해시나 체크섬을 사용해서 중복 콘텐츠를 쉽게 탐지
2. 거미 덫
    - 크롤러를 무한 루프에 빠뜨리도록 설계한 웹 페이지
    - URL의 최대 길이를 제한해서 회피
3. 데이터 노이즈
    - 가치가 없는 콘텐츠도 있기 때문에 가능한한 제외할 수 있도록 처리

# 10장 - 알림 시스템 설계
- 알림 시스템은 모바일 푸시 알림, SMS 메시지, 이메일 세 가지로 분류

## 1단계 - 문제 이해 및 설계 범위 확정
- 푸시 알림, SMS 메시지, 이메일 지원
- 연성 실시간(soft real-time) 시스템 <-> 경성 실시간(hard real-time)
- iOS 단말, 안드로이드 단말, 랩톱/데스크톱 지원
- 클라이언트 애플리케이션 또는 서버 측에서 스케줄링으로 알람 생성
- 알림 거부 설정 여부
- 하루에 보내야할 알림 횟수
    - 모바일 푸시 알림: 10,000,000건
    - SMS 메시지: 1,000,000건
    - 이메일: 5,000,000건

## 2단계 개략적 설계안 제시 및 동의 구하기

### 알림 유형별 지원 방안
- 각 알림 메커니즘이 어떻게 동작하는지부터 이해

#### iOS 푸시 알림
- iOS에서 푸시 알림을 보내기 위해서는 세 가지 컴포넌트가 필요
    + 알림 제공자: 알림 요청을 만들어 애플 푸시 알림 서비스(APNS)로 보내는 주체
    + APNS: 애플이 제공하는 원격 서비스. 푸시 알림을 iOS 장치로 보내는 역할을 담당
    + iOS 단말: 푸시 알림을 수신하는 사용자 단말

#### 안드로이드 푸시 알림
- 안드로이드 푸시 알림은 APNS 대신 FCM을 사용하는 점을 제외하면 APNS와 동일

#### SMS 메시지
- 보통 트윌리오, 넥스모 같은 제3 사업자의 서비스를 많이 이용
- 대부분 사용 서비스라서 이용요금을 내야 함

#### 이메일
- 대부분 이메일 서버를 구축할 역량은 있지만, 그럼에도 상용 이메일 서비스를 이용
- 대표적으로 센드그리드, 메일침프가 있음. 전송 성공률도 높고, 데이터 분석 서비스도 제공

### 연락처 정보 수집 절차
- 알림을 보내려면 모바일 단말 토큰, 전화번호, 이메일 주소 등의 정보가 필요
- 사용자가 앱을 처음 설치하거나 처음 계정을 등록하면 API 서버는 해당 사용자의 정보를 수집하여 데이터베이스에 저장

### 알림 전송 및 수신 절차
- 개략적 설계안(초안)
    + 서비스 -> 알림 시스템 -> 제3자 서비스 -> 단말
    + 1부터 N까지의 서비스: 알림 시스템으로 알림 전송 요청을 보내는 시스템의 서비스
    + 알림 시스템: 알림 전송/수신 처리의 핵심
    + 제3자 서비스: 사용자에게 알림을 실제로 전달하는 역할을 함
    + iOS, 안드로이드, SMS, 이메일 단말: 사용자가 이메일을 수신할 때 사용하는 단말
- 위 설계의 문제
    + SPOF: 알림 서비스에 서버가 하나밖에 없다는 것은 알림 시스템에 장애가 생기면 전체 서비스의 장애로 이어짐
    + 규모 확장성: 한 대 서비스로 푸시 알림에 관계된 모든 것을 처리하므로, 데이터베이스나 캐시 등 중요 컴포넌트의 규모를 개별적으로 늘릴 방법이 없다
    + 성능 병목: 알림을 처리하고 보내는 것은 자원을 많이 필요로 하는 작업. 한 서버로 처리하면 사용자 트래픽이 많이 몰리는 시간에는 시스템이 과부하 상태에 빠질 수 있음

- 개략적 설계안(개선된 버전)
    + 데이터베이스와 캐시를 알림 시스템의 주 서버에서 분리해 별도로 관리
    + 알림 서버를 증설하고 자동으로 수평적 규모 확장이 이루어질 수 있도록 한다
    + 메시지 큐를 이용해 시스템 컴포넌트 사이의 강한 결합을 끊음
    + 메시지 큐에서 전송할 알림을 꺼내서 제3자 서비스로 전달하는 작업 서버 추가
- 컴포넌트들이 협력하여 알림을 전송하게 되는 방법
    1. API를 호출하여 알림 서버로 알림을 전송
    2. 알림 서버는 사용자 정보, 단말 토큰, 알림 설정 같은 메타데이터를 캐시나 데이터베이스에서 가져옴
    3. 알림 서버는 전송할 알림에 맞는 이벤트를 만들어서 해당 이벤트를 위한 큐에 넣음
    4. 작업 서버는 메시지 큐에서 알림 이벤트를 꺼냄
    5. 작업 서버는 알림을 제3자 서비스로 보냄
    6. 제3자 서비스는 사용자 단말기로 알림을 전송
## 3단계 - 상세 설계
- 상세 설계에서 아래 내용을 좀 더 자세히 알아봄
    + 안정성(reliability)
    + 추가로 필요한 컴포넌트 및 고려사항: 알림 템플릿, 알림 설정, 전송률 제한, 재시도 메커니즘, 보안, 큐에 보관된 알림에 대한 모니터링과 이벤트 추적

### 안정성
- 분산 환경에서 운영될 알림 시스템을 설계할 때는 안정성을 확보하기 위한 사항 몇 가지를 반드시 고려

#### 데이터 손실 방지
- 가장 중요한 요구사항 가운데 하나는 어떤 상황에서도 알림이 소실되면 안 된다는 것
- 알림이 지연되거나 순서가 틀려도 괜찮지만 사라지면 곤란함
- 그래서 알림 데이터를 데이터베이스에 보관하고 재시도 메커니즘을 구현해야 됨

#### 알림 중복 전송 방지
- 같은 알람이 여러 번 전송되는 것을 완전히 막는 것은 불가능
- 분산 시스템의 특성상 가끔은 같은 알림이 중복되어 전송됨
- 간단하게 알림 중복을 검사하려면 알림에 ID를 부여하고 이전에 전송된 이벤트인지 보고 전송 여부 결정

### 추가로 필요한 컴포넌트 및 고려사항
- 알림 시스템은 위 내용 보다 훨씬 복잡하기 때문에 알림 템플릿, 알림 설정, 이벤트 추적, 시스템 모니터링, 처리율 제한 등 알림 시스템 구현을 위해 필요한 추가 컴포넌트을 자세히 보자

#### 알림 템플릿
- 대형 알림 시스템은 하루에 수백만 건 이상의 알림을 처리
- 대부분의 알림 메시지는 형식이 비슷해서 모든 부분을 처음부터 다시 만들 필요 없도록 해줌

#### 알림 설정
- 사용자는 대부분 많은 알림을 받고 있어서 쉽게 피곤함을 느낌
_ 따라서 사용자가 알림 설정을 상세히 조정할 수 있도록 함

#### 전송률 제한
- 한 사용자가 받을 수 있는 알림의 빈도를 제한
- 너무 많은 알림을 보내면 사용자가 알림을 아예 꺼 버릴 수도 있음

#### 재시도 방법
- 제3자 서비스가 알림 전송에 실패하면, 해당 알림을 재시도 전용 큐에 넣음

#### 푸시 알림과 보안
- iOS와 안드로이드 앱의 경우 알림 전송 API는 appKey와 appSecret을 사용하여 보안 유지

#### 큐 모니터링
- 알림 시스템을 모니터링 할 때 중요한 메트릭중 하나는 큐에 쌓인 알람의 개수임
- 알림 개수를 보며 작업 서버를 조절

#### 이벤트 추적
- 알림 확인율, 클릭율, 실제 앱 사용으로 이어지는 비율 같은 메트릭이 사용자를 이해하는데 중요함

# 11장 - 뉴스 피드 시스템 설계
- 뉴스 피드는 우리 홈 페이지 중앙에 지속적으로 업데이트되는 스토리들

## 1단계 - 문제 이해 및 설계 범위 확정
- 모바일 앱, 웹 둘 다 지원
- 사용자가 뉴스 피드 페이지에 새로운 스토리를 올릴 수 있어야 하고, 친구들이 올리는 스토리를 볼 수도 있어야 함
- 포스팅은 시간 흐름 역순으로 정렬
- 한 명의 사용자는 최대 5,000명의 친구를 가질 수 있음
- 트래픽 규모는 매일 천만 명이 방문
- 피드에는 이미지나 비디오도 포함 가능

## 2단계 - 개략적 설계안 제시 및 동의 구하기
- 크게 설계안은 피드 발행과 뉴스 피드 생성으로 나뉨
    + 피드 발행: 사용자가 스토리를 포스팅하면 해당 데이터를 캐시와 데이터베이스에 기록
    + 뉴스 피드 생성: 지면 관계상 뉴스 피드는 모든 친구의 포스팅을 시간 흐름 역순으로 모아서 만든다고 가정

### 뉴스 피드 API
- HTTP 기반으로 상태 정보 업데이트, 뉴스 피드 가져오기, 친구 추가 등의 다양한 작업을 수행하는데 사용

#### 피드 발행
- 사용자: 모바일 앱이나 브라우저에서 새 포스팅을 올리는 주체
- 로드밸런서: 트래픽들을 웹 서버들로 분산
- 포스팅 저장 서비스: 새 포스팅을 데이터베이스와 캐시에 저장
- 포스팅 전송 서비스: 새 포스팅을 친구의 뉴스 피드에 푸시. 뉴스 피드 데이터는 캐시에 보관하여 빠르게 읽을 수 있도록 처리
- 알림 서비스: 새 포스팅이 올라왔음을 알림

#### 뉴스 피드 생성
- 사용자: 뉴스 피드를 읽는 주체
- 로드 밸런서: 트래픽을 웹 서버들로 분산
- 웹 서버: 트래픽을 뉴스 피드 서비스로 보냄
- 뉴스 피드 서비스: 캐시에서 뉴스 피드를 가져옴
- 뉴스 피드 캐시: 뉴스 피드를 렌더링할 때 필요한 피드 ID를 보관

## 3단계 - 상세 설계
- 뉴스 피드 발행과 생성의 두 가지 설계를 보다 상세히 정리

### 피드 발행 흐름 상세 설계
- 컴포넌트 자체는 개략적 설계안에서 다룬 정도로 충분
- 웹 서버와 포스팅 전송 서비스에 초점

#### 웹 서버
- 클라이언트와 통신, 인증 처리, 처리율 제한 등의 기능도 수행

#### 포스팅 전송(팬아웃) 서비스
- 포스팅 전송(fanout)은 어떤 사용자의 새 포스팅을 그 사용자와 친구 관계에 있는 모든 사용자에게 전달하는 과정
- 팬아웃에는 두 가지 모델이 있음
    + 쓰기 시점에 팬아웃(push 모델)
    + 읽기 시점에 팬아웃(pull 모델)
- 쓰기 시점에 하는 팬아웃: 새로운 포스팅을 기록하는 시점에 뉴스 피드를 갱신
    + 장점
        1. 뉴스 피드가 실시간으로 갱신되며 친구 목록 사용자에게 즉시 전송
        2. 새 포스팅이 기록되는 순간에 뉴스 피드가 갱신 되므로 뉴스 피드를 읽는데 드는 시간이 짧아짐
    + 단점
        1. 친구가 많은 사용자의 경우 친구 목록을 가져오고 해당 목록에 있는 사용자 모두의 뉴스 피드를 갱신하는 데 많은 시간이 소요
        2. 서비스를 자주 이용하지 않는 사용자의 피드까지 갱신해야 하므로 컴퓨팅 자원이 낭비됨
- 읽기 시점에 팬아웃: 피드를 읽어야 하는 시점에 뉴스 피드를 갱신
    + 장점
        1. 비활성화된 사용자에 대한 자원 소모가 없음
        2. 데이터를 모든 친구들에게 푸시하는 작업이 필요 없으므로 핫키 문제가 발생하지 않음
    + 단점
        1. 뉴스 피드를 읽는 데 많은 시간이 소요될 수 있음
- 해당 설계안에서는 두 가지 방법을 결합하여 장점은 취하고 단점은 버림
    + 대부분의 사용자에 대해서는 푸시 모델 선택
    + 친구가 아주 많은 사용자의 경우에는 팔로어로 하여금 피드를 가져올 때 풀 모델을 사용하여 시스템 과부하 방지
    + 추가로 안정 해시를 통해 요청과 데이터를 보다 고르게 분산하여 핫키 문제를 줄임

# 12장 - 채팅 시스템 설계
- 채팅 시스템은 종류가 제각각이기 때문에 요구사항을 확실하게 해야됨

## 1단계 - 문제 이해 및 설계 범위 확정
- 1:1과 그룹 채팅 모두 지원
- 모바일 앱과 웹 모두 지원
- 트래픽은 일별 능동 사용자 수 기준으로 5천만 명 처리 가능 해야함
- 그룹 채팅은 최대 100명까지 참가
- 중요 기능
    + 1:1 채팅
    + 그룹 채팅
    + 사용자 접속상태 표시
    + 텍스트 메시지만 주고 받을 수 있음
- 메시지 길이는 100,000자 이하
- 종단 간 암호화는 필요 없음
- 채팅 이력은 영원히 보관

## 2단계 - 개략적 설계안 제시 및 동의 구하기
- 클라이언트가 직접 통신하지 않기 때무넹 클라이언트와 서버의 통신 방법에 대한 기본적인 지식이 있어야 됨
- 채팅 서비스는 아래 기능을 제공해야 됨
    + 클라이언트들로부터 메시지 수신
    + 메시지 수신자 결정 및 전달
    + 수신자가 접속 상태가 아닌 경우에는 접속할 때까지 해당 메시지 보관
- 메시지 송신 클라이언트 -> 채팅 서비스(메시지 저장 및 전달 -> 메시지 수신 클라이언트
- 메시지 전송에는 HTTP 메시지, 폴링, 롱 폴링, 웹소켓 등을 사용할 수 있다

### 폴링
- 클라이언트가 주기적으로 서버에게 새 메시지가 있는지 확인하는 방법
- 폴링은 받을 메시지가 없는 경우 서버 자원이 불필요하게 낭비되는 문제 발생

### 롱 폴링
- 폴링이 여러 가지로 비효울적이라 나온 기법
- 클라이언트가 새 메시지를 받거나 타임아웃 될 때까지 연결을 유지
- 새 메시지를 받으면 기존 연결을 종료하고 새로운 요청을 보내어 위 절차를 다시 시작
- 약점
    + 메시지를 보내는 클라이언트와 수신하는 클라이언트가 같은 채팅 서버에 접속하게 되지 않을 수도 있음
    + 서버 입장에서는 클라이언트가 연결을 해제했는지 아닌지 알 방법이 없음
    + 여전히 비효율적임. 메시지를 많이 받지 않는 클라이언트도 타임아웃이 일어날 때마다 주기적으로 서버에 다시 접속

### 웹소켓
- 서버가 클라이언트에게 비동기 메시지를 보낼 때 가장 널리 사용하는 기술
- 연결은 클라이언트가 시작
- 한번 맺어진 연결은 항구적이며 양방향이다
- 웹소켓은 첫 연결은 HTTP 이지만 특정 핸드셰이크 절차를 거쳐 웹소켓 연결로 업그레이드 됨
- 서버 측에서 연결 관리를 효율적으로 해야 함

### 개략적 설계안
- 클라이언트와 서버 사이의 주 통신 프로토콜로 웹소켓을 사용하기로 했지만 대부분의 기능은 일반적인 HTTP 상에서 구현해도 됨
- 채팅 시스템은 세 부분으로 나누어 볼 수 있음
    + 무상태 서비스
    + 상태유지 서비스
    + 제3자 서비스 연동

#### 무상태 서비스
- 무상태 서비스는 로그인, 회원가입, 사용자 프로파일 표시 등을 처리하는 전통적인 요청/응답 서비스
- 무상태 서비스는 로드밸런서 뒤에 위치, 로드밸런서는 요청을 그 경로에 맞는 서비스로 정확하게 전달

#### 상태 유지 서비스
- 해당 설계안에서 상태 유지가 필요한 서비스는 채팅 서비스임
- 각 클라이언트가 채팅 서버와 독립적인 네트워크 연결을 유지해야 됨

#### 제3자 서비스 연동
- 채팅 앱에서 가장 중요한 제3자 서비스는 푸시 알림

#### 규모 확장성
- 트래픽 규모가 얼마 되지 않을 때는 모든 기능을 서버 한 대로 구현할 수 있다
- 하지만 트래픽이 커진다면 서버 한대에 모든 기능을 담는건 위험함
    + SPOF가 될 수 있음
    + 트래픽이 몰릴때 대처가 안됨

#### 저장소
- 기술 스택 깊은 곳에 데이터 계층이 있고, 데이터 계층을 올바르게 만드는 데는 노력이 필요함
- 일반적인 사용자 프로파일, 설정, 친구 목록 같은 데이터는 관계형 데이터베이스에 저장
- 채팅 이력은 데이터가 어마어마하고 읽기:쓰기 비율이 대략 1:1 정도이기 때문에 키-값 저장소 추천
    + 키-값 저장소는 수평적 규모확장이 쉽다
    + 키-값 저장소는 데이터 접근 지연시간이 낮다
    + 관계형 데이터베이스는 데이터 가운데 롱 테일에 해당하는 부분을 잘 처리하지 못하는 경향이 있음. mysql의 offset 문제
    + 이미 많은 안정적인 채팅 시스템이 키-값 저장소를 채택

## 3단계 - 상세 설계
- 채팅 시스템에서는 서비스 탐색, 메시지 전달 흐름, 사용자 접속 상태 표시 방법 정도가 자세히 살펴볼 만한 부분

### 서비스 탐색
- 클라이언트에게 가장 적합한 채팅 서버를 추천
- 많이 사용되는 오픈 소스 솔루션으로는 아파치 주키퍼 같은 것이 있음
    + 사용 가능한 모든 채팅 서버를 주키퍼에 등록하고, 클라이언트가 접속을 시도하면 사전에 정한 기준에 따라 최적의 채팅 서버를 선택
- 서비스 탐색 흐름
    1. 사용자가 시스템에 로그인을 시도
    2. 로드밸런서가 로그인 요청을 API 서버로 전달
    3. API 서버가 사용자 인증을 처리하고 서비스 탐색 기능이 동작하여 최적의 채팅 서버를 찾음
    4. 사용자는 선택된 채팅 서버에 연결

### 메시지 흐름
- 채팅 시스템에서 종단 간 메시지 흐름을 이해하는 것은 중요함

#### 1:1 채팅 메시지 처리 흐름
1. 사용자 A가 채팅 서버로 메시지 전송
2. 채팅 서버는 ID 생성기를 사용해 해당 메시지의 ID 결정
3. 채팅 서버는 해당 메시지를 메시지 동기화 큐로 전송
4. 메시지가 키-값 저장소에 보관됨
5. 사용자 B가 접속중인 경우에는 채팅 서버로 전송하고, 접속 중이 아니라면 푸시 알림 메시지를 보냄
6. 채팅 서버는 메시지를 사용자 B에게 전송. 사용자 B와 연결된 웹소켓 연결 사용

#### 여러 단말 사이의 메시지 동기화
- 한 명의 사용자가 여러 개의 단말을 사용하는 경우가 있음
- 각 단말은 cur_max_message_id라는 변수를 유지. 해당 단말에서 관측된 가장 최신 메시지의 ID를 추적하는 용도

### 접속상태 표시
- 접속상태 서버를 통해 사용자의 상태를 관리
- 접속상태 서버는 클라이언트와 웹소켓으로 통신하는 실시간 서비스임

#### 사용자 로그인
- 클라이언트와 실시간 서비스 사이에 웹소켓 연결이 맺어지면 사용자에 대한 상태와 last_active_at 타임스탬프 값을 키-값 저장소에 보관

#### 로그아웃
- API 서버로 로그아웃을 요청하면 접속상태 서버로 로그아웃 요청을 보내고, 키-값 저장소에 상태가 로그아웃으로 등록된다

#### 접속 장애
- 인터넷 연결이 좋지 못한 상황에 대응할 수 있는 설계를 준비해야 됨
- 간단한 방법은 사용자를 오프라인 상태로 표시
- 좀더 좋은 방법은 heartbeat 검사를 통해 문제 해결
    + 클라이언트로부터 계속 heartbeat 이벤트를 받고 일정 시간동안 오지 않으면 상태를 오프라인으로 바꾸는 방법

#### 상태 정보의 전송
- 상태정보 서버는 발행-구독 모델을 사용
- A 사용자가 로그아웃 하면 B, C, D 친구의 채널에 이벤트 전송
- 위 방법은 그룹 크기가 작을때 사용할 수 있다
- 그룹 크기가 커지면 상태 정보를 수동으로 갱신하도록 유도

# 13장 - 검색어 자동완성 시스템
- 입력 중인 글자에 맞는 검색어가 자동으로 완성되는 기능
- 가장 많이 사용되는 검색어 k개를 자동완성하여 출력하는 시스템을 설계

## 1단계 - 문제 이해 및 설계 범위 확정
- prefix 검색으로 한정
- 최대 5개의 자동완성 검색어 표시
- 순위는 질의 빈도에 따라 정해지는 검색어 인기 순위를 기준
- 맞춤법 검사나 자동수정은 지원하지 않음
- 영어만 해도 되고 다국어도 해도 됨
- 모든 질의는 영어 소문자로 처리
- DAU 기준으로 천만명의 트래픽

### 요구사항
- 빠른 응답 속도: 사용자가 검색어를 입력함에 따라 자동완성 검색어도 충분히 빨리 표시되어야 한다
    + 페이스북 검색어 자동완성 시스템에 관한 문서를 보면 시스템 응답 속도는 100 밀리초 이내
- 연관성: 자동완성되어 출력되는 검색어는 사용자가 입력한 단어와 연관된것이어야 함
- 정렬: 계산 결과는 인기도 등의 순위 모델에 의해 정렬되어 있어야 함
- 규모 확장성: 시스템은 많은 트래픽을 감당할 수 있도록 확장 가능해야 함
- 고가용성: 시스템의 일부에 장애가 발생하거나 느려지거나, 예상치 못한 네트워크 문제가 생겨도 시스템은 계속 사용 가능해야 함

### 개략적 규모 추정
- DUA는 천만명으로 가정
- 평균적으로 한 사용자는 매일 10건의 검색을 수행
- 질의 할때마다 평균적으로 20바이트의 데이터를 입력
- 검색착에 글자를 입력할 때마다 클라이언트는 검색어 자동완성 백엔드에 요청을 보냄
    + 평균적으로 1회 검색당 20건의 요청이 백엔드로 전달
- 대략 초당 24,000건의 질의(QPS)가 발생
- 최대 QPS=QPS x 2 = 대략 48,000
- 질의 가운데 20% 정도는 신규 검색이라고 가정
    + 대략 0.4GB의 신규 데이터가 시스템에 추가

## 2단계 - 개략적 설계안 제시 및 동의 구하기
-  해당 시스템은 개략적으로 두 부분으로 나뉨
    + 데이터 수집 서비스: 사용자가 입력한 질의를 실시간으로 수집. 데이터가 많은 애플리케이션에서 실시간 시스템은 바람직하지 않음
    + 질의 서비스: 주어진 질의에 다섯 개의 인기 검색어를 정렬해 내놓는 서비스

### 데이터 수집 서비스
- 질의문과 사용빈도를 저장하는 빈도 테이블이 있다 가정
- 사용자가 질의하면 질의한 값에 빈도를 1씩 증가

### 질의 서비스
- 데이터 수집에서 생서된 빈도 테이블이 있는 상태
    + query: 질의문을 저장하는 필드
    + frequency: 질의문이 사용된 빈도를 저장하는 필드
- 간단하게 데이터베이스에서 preifx 검색으로 처리할 수 있지만 데이터가 아주 많아지면 데이터베이스가 병목이 될 수 있다

## 3단계 - 상세 설계
- 이번 절에서는 컴포넌트를 몇개 골라 보다 상세히 설계하고 다음 순서로 최적화 방안을 논의
    + 트라이(trie) 자료구조
    + 데이터 수집 서비스
    + 질의 서비스
    + 규모 확장이 가능한 장소
    + 트라이 연산

### 트라이 자료구조
- 트라이(trie): 접두어 트리(prefix tree) 라고도 함
- 트라이는 문자열들을 간략하게 저장할 수 있는 자료 구조
    + 트라이는 트리 형태의 자료구조
    + 트리의 루트 노드는 빈 문자열을 나타냄
    + 각 노드는 글자 하나를 저장하며 26개의 자식 노드를 가질 수 있음
    + 각 트리 노드는 하나의 단어 또는 접두어 문자열을 타나냄
- 트라이 자료구조에 빈도 정보까지 기록
- 용어 정의
    + p: 접두어의 길이
    + n: 트라이 안에 있는 노드 개수
    + c: 주어진 노드의 자식 노드 개수
- 가장 많이 사용된 질의어 k개를 찾는 방법
    + 해당 접두어를 표현하는 노드를 찾음. 시간 복잡도는 O(p)임
    + 해당 노드부터 시작하는 하위 트리를 탐색하여 모든 유효 노드를 찾음. 유효한 검색 문자열을 구성하는 노드가 유효 노드임. 시간 복잡도는 O(c)
    + 유효 노드들을 정렬하여 가장 인기 있는 검색어 k개를 찾음. 시간 복잡도는 O(c log c)
- 위 알고리즘은 직관적이지만 최악의 경우에는 k개 결과를 얻으려고 전체 트라이를 다 검색해야 하는 일이 발생. 해당 문제 해결 방법으로는 아래 두가지가 있음
    + 접두어의 최대 길이 제한
    + 각 노드에 인기 검색어를 캐시

#### 접두어 최대 길이 제한
- 검색어 최대 길이를 제한할 수 있다면 접두어 노드를 찾는 단계의 시간 복잡도는 O(p)에서 O(작은 상숫값)=O(1)로 바뀔 것

#### 노드에 인기 검색어 캐시
- 각 노드에 k개의 인기 검색어를 저장해 두면 전체 트라이를 검색하는 일을 방지할 수 있음
- 단점은 각 노드에 질의어를 저장할 공간이 많이 필요하게 됨
- 앞의 두 가지 최적화 기법을 사용하면 시간 복잡도가 O(1)로 바뀌게 됨

### 데이터 수집 서비스
- 앞의 설계안에서는 사용자가 검색창에 뭔가 타이핑을 할 때마다 실시간으로 데이터를 수정
- 위 방법은 다음 두 가지 문제로 실용적이지 못함
    + 매일 수천만 건의 질의가 입력될 텐데 그때마다 트라이를 갱신하면 질의 서비스는 심각하게 느려질것
    + 일단 트라이가 만들어지고 나면 인기 검색어는 그다지 자주 바뀌지 않을 것. 그래서 트라이를 그렇게 자주 갱신할 필요가 없음
- 데이터 분석 서비서의 수정된 설계안
    + 데이터 분석 서비스 로그 -> 로그 취합 서버 -> 취합된 데이터 -> 작업 서버 -> 트라이 데이터베이스 -> 트라이 캐시

#### 데이터 분석 서비스 로그
- 검색창에 입력된 질의에 관한 원본 데이터가 보관
- 새로운 데이터가 추가될 뿐 수정은 이루어지지 않으므로 인덱스는 걸지 않음

#### 로그 취합 서버
- 로그는 보통 양이 엄청나고 데이터 형식도 제각각인 경우가 많아서 데이터를 잘 취합하여(aggregation) 우리 시스템이 쉽게 소비할 수 있도록 해야 함
- 데이터 취합 방식은 서비스의 용례에 따라 달라질 수 있음
    + 트위터 같은 실시간 애플리케이션의 경우에는 데이터 취합 주기를 보다 짧게 가져갈 필요가 있음
    + 대부분의 경우에는 일주일에 한 번 정도 로그를 취합해도 충분할 것

#### 취합된 데이터
- 로그를 통해 취합된 데이터는 검색된 값과 시간 횟수 합 등이 포함된 데이터

#### 작업 서버
- 작업 서버는 주기적으로 비동기적 작업을 실행하는 서버 집합
- 트라이 자료구조를 만들고 트라이 데이터베이스에 저장하는 역할을 담당

#### 트라이 캐시
- 분산 캐시 시스템으로 트라이 데이터를 메모리에 유지하여 읽기 연산 성능을 높이는 구실을 함

#### 트라이 데이터베이스
- 지속성 저장소
- 선택지로는 아래 두가지가 있음
    + 문서 저장소: 새 트라이를 매주 만들거이므로 주기적으로 트라이를 직렬화하여 데이터베이스에 저장할 수 있음. 몽고디비 같은 문서 저장소를 활용하면 이런 데이터를 편리하게 저장할 수 있음
    + 키-값 저장소: 트라이는 해시 테이블 형태로 변환 가능하다

#### 질의 서비스
- 아래 설계안은 비효율성을 개선한 새 설계안
    1. 검색 질의가 로드밸런서로 전송
    2. 로드밸런서는 해당 질의를 API 서버로 전송
    3. API 서버는 트라이 캐시에 데이터를 가져와 해당 요청에 대한 자동완성 검색어 제안 응답을 구성
    4. 데이터가 트라이 캐시에 없는 경우 데이터베이스에서 가져와 채움. 캐시 미스는 캐시 서버의 메모리가 부족하거나 캐시 서버에 장애가 있어도 발생

### 트라이 연산
- 트라이는 검색어 자동완성 시스템의 핵심 컴포넌트

#### 트라이 생성
- 트라이 생성은 작업 서버가 담당.
- 데이터 분석 서비스의 로그나 데이터베이스로부터 취합된 데이터를 이용

#### 트라이 갱신
- 갱신하는 두 가지 방법
    1. 매주 한 번 갱신하는 방법
    2. 트라이의 각 노드를 개별적으로 갱신
        + 이 방법은 성능이 좋지 않음

#### 검색어 삭제
- 혐오성이 짙거나, 폭력적이거나, 성적으로 노골적이거나, 여러 가지로 위험한 질의어를 자동완성 결과에서 제거
- 좋은 방법은 처리 서버와 캐시 사에서 필터 계층을 넣어서 처리
- 물리적인 삭제는 다음번 업데이트 사이클에 비동기적으로 진행하면 됨

### 저장소 규모 확장
- 트라이의 크기가 한 서버에 넣기엔 너무 큰 경우에도 대응할 수 있도록 규모 확장성 문제를 해결
- 영어만 지원하면 되기 때문에, 간단하게는 첫 글자를 기준으로 샤딩
    + 해당 방법은 언어별 시작하는 단어수가 달라서 균등하게 배분하기가 불가능
- 그래서 과거 질의 데이터의 패턴을 분석하여 샤딩
    + s로 시작하는 검색어의 양이, u,v,w,x,y,z로 시작하는 검색어를 전부 합친 것과 비슷하다면 s에 대한 샤드 하나와 u-z에 대한 샤드 하나를 두면 충분

# 14장 - 유튜브 설계
- 유튜브 시스템은 엄청나게 복잡한 수많은 기술이 숨어 있음
- 유튜브의 통계 자료
    + MAU: 20억
    + 매일 재생되는 비디오 수: 50억
    + 미국 성인 가운데 73%가 유튜브 이용
    + 5천만 명의 창작자
    + 유튜브 광고 수입은 2019년 기준으로 150억 달러
    + 모바일 인터넷 트래픽 가운데 37%를 유튜브가 점유
    + 80개 언어로 이용 가능

## 1단계 - 문제 이해 및 설계 범위 확정
- 유튜브에서는 단순히 비디오를 보는 것 말고도 많은 일을 할 수 있음
    + 댓글을 남기거나, 비디오를 공유하거나, 좋아요 버튼을 누를 수도 있음, 자기 재생 목록에 저장, 채널을 골라 구독
- 그래서 설계 범위를 좁히는게 중요함
    + 어떤 기능이 가장 중요한가?
        - 비디오를 올리는 기능과 시청하는 기능
    + 어떤 클라이언트를 지원해야 하나?
        - 모바일 앱, 웹 브라우저, 스마트 TV
    + 일간 능동 사용자 수는 몇 명?
        - 500만명
    + 사용자가 이 제품에 평균적으로 소비하는 시간
        - 30분
    + 다국어 지원 여부
        - 어떤 언어로도 이용 가능
    + 비디오 해상도 종류
        - 현존하는 비디오 종류와 해상도를 대부분 지원
    + 암호화 여부
        - 필요
    + 비디오 파일 크기 제한
        - 작은 비디오, 혹은 중간 크기 비디오에 초점을 맞춤. 최대 크기는 1GB로 제한
    + AWS, Azure, GCP 등 이용 가능한가?
        + 활용할 수 있다면 하는 것이 바람직 함
- 아래 기능을 갖는 비디오 스트리밍 서비스 설계에 초점
    + 빠른 비디오 업로드
    + 원활한 비디오 재생
    + 재생 품질 선택 가능
    + 낮은 인프라 비용
    + 높은 가용성과 규모 확장성, 안정성
    + 지원 클라이언트: 모바일 앱, 웹브라우저, 스마트 TV

### 개략적 규모 추정
- 일간 능동 사용자: 500만
- 사용자별 하루 평균 5개 비디오 시청
- 10%의 사용자가 하루에 1개 비디오 업로드
- 비디오 평균 크기는 300MB
- 비디오 저장을 위해 매일 새로 요구되는 저장 용량=500만x10%x300MB=150TB
- CDN 비용
    + 클라우드 CDN을 통해 비디오를 서비스할 경우 CDN에서 나가는 데이터의 양에 따라 과금
    + AWS 기준으로 100% 트래픽이 미국에서 발생하면 1GB당 $0.02의 요금이 발생
    + 매일 발생하는 요금: 500만x5비디오x0.3GBx$0.02=%150,000
    + CDN을 통해 비디오를 서비스하면 비용이 엄청크기 때문에 상세 설계에서 비용 줄이는 방법에 대해 진행

## 2단계 - 개략적 설계안 제시 및 동의 구하기
- 여기서 제시하는 설계안은 CDN과 BLOB 스토리지의 경우에는 기존 클라우드 서비스 사용
    + 직접 만드는것은 시스템 면접에서 관계가 없음. 그 기술 각각이 어떻게 동작하는지 설명하는것보다 적절한 기술을 골라 시간안에 설계를 마치는 것이 중요
    + 실제로 큰 회사도 모든것을 스스로 구축하지는 않는다
- 개략적으로 시스템은 아래 세 개 컴포넌트로 구성
    + 단말(Client): 컴퓨터, 모바일 폰, 스마트 TV를 통해서 유튜브를 시청
    + CDN: 비디오는 CDN에 저장, 재생 버튼을 누르면 CDN으로부터 스트리밍이 이루어진다
    + API 서버: 비디오 스트리밍을 제외한 모든 요청은 API 서버가 처리
- 면접관이 아래 두 영역을 설계해 줄 것을 요청했다고 가정
    + 비디오 업로드 절차
    + 비디오 스트리밍 절차

### 비디오 업로드 절차
- 사용자: 컴퓨터, 모바일 폰, 스마트 TV를 통해 유튜브를 시정하는 이용자
- 로드밸런서: API 서버 각각으로 고르게 요청을 분산하는 역할을 담당
- API 서버: 비디오 스트리밍을 제외한 다른 모든 요청을 처리
- 메타데이터 데이터베이스: 비디오의 메타데이터를 보관. 샤딩과 다중화를 적용하여 성능 및 가용성 요구사항 충족
- 메타데이터 캐시: 성능을 높이기 위해 비디오 메타데이터와 사용자 객체는 캐시
- 원본 저장소: 원본 비디오를 보관할 대형 이진 파일 저장소(BLOB, Binary Large Object Storage) 시스템
- 트랜스 코딩 서버: 비디오 트랜스코딩은 비디오 인코딩이라 부르기도 함. 비디오의 포맷을 변환하는 절차. 최적의 비디오 스트림을 제공하기 위해 필요
- 트랜스코딩 비디오 저장소: 트랜스코딩이 완료된 비디오를 저장하는 BLOB 저장소
- CDN: 비디오를 캐시하는 역할을 담당. 사용자가 재생 버튼을 누르면 비디오 스트리밍은 CDN을 통해 이루어 짐
- 트랜스코딩 완료 큐: 비디오 트랜스코딩 완료 이벤트들을 보관할 메시지 큐
- 트랜스코딩 완료 핸들러: 트랜스코딩 완료 큐에서 이벤트 데이터를 꺼내어 메타데이터 캐시와 데이터베이스를 갱신할 작업 서버들

#### 프로세스 A: 비디오 업로드
1. 비디오를 원본 저장소에 업로드 한다
2. 트랜스코딩 서버는 원본 저장소에서 해당 비디오를 가져와 트랜스코딩을 시작
3. 트랜스코딩이 완료되면 아래 두 절차가 병렬적으로 수행
    3-1. 완료된 비디오를 트랜스코딩 비디오 저장소로 업로드
    3-2. 트랜스코딩 완료 이벤트를 트랜스코딩 완료 큐에 넣음
        3-1-1. 트랜스코딩이 끝난 비디오를 CDN에 올림
        3-2-1. 완료 핸들러가 이벤트 데이터를 큐에서 꺼냄
        3-2-1-1, 3-2-1-2. 완료 핸들러가 메타데이터 데이터베이스와 캐시를 갱신
4. API 서버가 단말에게 비디오 업로드가 끝나서 스트리밍 준비가 되었음을 알림

#### 프로세스 B: 메타데이터 갱신
- 원본 저장소에 파일이 업로드되는 동안, 단말은 병렬적으로 비디오 메타데이터 갱신 요청을 API 서버에 보냄
    + 메타데이터에는 파일 이름, 크기, 포맷 등의 정보가 들어 있음

### 비디오 스트리밍 절차
- 스트리밍은 장치가 원격지의 비디오로부터 지속적으로 비디오 스트림을 전송 받아 영상을 재생하는 것을 말함
- 비디오 스트리밍이 이루어지는 절차를 논하려면 스트리밍 프로토콜이라는 개념을 알아야 함
```
비디오 스트리밍
비디오 스트리밍을 위해 데이터를 전송할 때 쓰이는 표준화된 통신 방법
- MPEG-DASH. Moving Picture Experts Group Dynamic Adaptive Streaming HTTP
- 애플 HLS, HTTP Live Streaming
- 마소 스무드 스트리밍
- 어도비 HTTP 동적 스트리밍
```
- 프로토콜마다 지원하는 비디오 인코딩이 다르고 플레이어도 다름. 따라서 서비스의 용례에 맞는 프로토콜을 잘 골라야 함
- 비디오는 CDN에서 바로 스트리밍 됨

## 3단계 - 상세 설계
- 비디오 업로드와 스트리밍을 담당하눈 부분을 최적화 방안과 함께 상세히 다듬고 오류 처리 메커니즘에 대해서 소개

### 비디오 트랜스코딩
- 비디오를 녹화하면 단말은 해당 비디오를 특정 포맷으로 저장. 이 비디오가 다른 단말에서도 재생되려면 다른 단말과 호환되는 비트레이트(bitrate)와 포맷으로 저장되어야 함
    + 비트레이트: 비디오를 구성하는 비트가 얼마나 빨리 처리되어야 하는지를 나타내는 단위
    + 비트레이트가 높으면 일반적으로 고화질 비디오
- 비디오 트랜스코딩이 중요한 이유
    + 가공되지 않은 원본 비디오는 저장 공간을 많이 차지함
    + 상당수의 단말과 브라우저는 특정 종류의 비디오 포맷만 지원. 호환성 문제를 해결하려면 하나의 비디오를 여러 포맷으로 인코딩해두는 것이 바람직 함
    + 사용자에게 끊김 없는 고화질 비디오 재생을 보장하려면, 네트워크 대역폭이 충분하지 않은 사용자에게는 저화질 비디오를, 대역폭이 충분한 사용자에게는 고화질 비디오를 보내는 것이 바람직 함
    + 모바일 단말의 경우 네트워크 상황이 수시로 달라질 수 있음. 비디오가 끊김 없이 재생되도록 하기 위해서는 비디오 화질을 자동으로 변경하거나 수동으로 변경할 수 있도록 해야 함
- 인코딩 포맷은 아주 다양하지만 대부분은 아래 두 부분으로 구성
    + 컨테이너: 비디오 파일, 오디오, 메타데이터를 담는 바구니 같은 것. 컨테이너 포맷은 avi, mov, mp4 같은 파일 확장자를 보면 알 수 있음
    + 코덱: 비디오 화질은 보존하면서 파일 크기를 줄일 목적으로 고안된 압축 및 압축 해제 알고리즘. 가장 많이 사용되는 비디오 코텍은 H.264, VP9, HEVC가 있음

### 유향 비순환 그래프(DAG) 모델
- 각기 다른 유형의 비디오 프로세싱 파이프라인을 지원하는 한편 처리 과정의 병렬성을 높이기 위해서는 적절한 수준의 추상화를 도입하여 클라이언트 프로그래머로 하여금 실행할 작업을 손수 정의할 수 있도록 해야 함
- 페이스북에서는 유향 비순환 그래프 프로그래밍 모델을 도입, 작업을 단계별로 배열할 수 있도록 하여 해당 작업들이 순차적으로 또는 병렬적으로 실행될 수 있도록 함
- 원본 비디오는 일단 비디오, 오디오, 메타데이터의 세 부분으로 나뉘어 처리
    + 비디오 부분에 적용되는 작업
        + 검사: 좋은 품질의 비디오인지, 손상은 없는지 확인하는 작업
        + 비디오 인코딩: 비디오를 다양한 해상도, 코덱, 비트레이트 조합으로 인코딩하는 작업
        + 섬네일: 사용자가 업로드한 이미지나 비디오에서 자동 추출된 이미지로 섬네일을 만드는 작업
        + 워터마크: 비디오에 대한 식별정보를 이미지 위에 오버레이 형태로 띄워 표시하는 작업

### 비디오 트랜스코딩 아키텍처
- 본 설계안에서는 클라우드 서비스를 활용한 비디오 트랜스코딩 아키텍처 정의
- 전처리기 -> DAG 스케줄러 -> 자원 관리자 -> 작업 실행 서버 -> 인코딩된 비디오
    + 전처리기 -> 임시 저장소
    + 작업 실행 ㅅ버 -> 임시 저장소

#### 전처리기
- 전처리기가 하는 일
    1. 비디오 분할: 비디오 스트림을 GOP라고 불리는 단위로 쪼갬. 하나의 GOP는 독립적으로 재생 가능하며 보통 몇초 정도의 길이임
        + GOP(Group of Pictures): 특정 순서로 배열된 프레임 그룹
    2. DAG 생성: 클라이언트 프로그래머가 작성한 설정 파일에 따라 DAG를 만들어 냄
    3. 데이터 캐시: 전처리기는 분할된 비디오의 캐시이기도 함. 안정성을 높이기 위해 전처리기는 GOP와 메타데이터를 임시 저장소에 보관

#### DAG 스케줄러
- DAG 스케줄러는 DAG 그래프를 몇 개 단계로 분할한 다음, 각각을 자원 관리자의 작업 큐에 집어 넣음

#### 자원 관리자
- 자원 관리자는 자원 배분을 효과적으로 수행하는 역할을 담당
- 새 개의 큐와 작업 스케줄러로 구성
    + 작업 큐: 실행할 작업이 보관되어 있는 우선순위 큐
    + 작업 서버 큐: 작업 서버의 가용 상태 정보가 보관되어 있는 우선순위 큐
    + 실행 큐: 현재 실행 중인 작업 및 작업 서버 정보가 보관
    + 작업 스케줄러: 최적의 작업/서버 조합을 골라, 해당 작업 서버가 작업을 수행하도록 지시하는 역할을 담당
- 작업 스케줄러 동작 방식
    + 작업 큐에서 가장 높은 우선순위의 작업을 꺼냄
    + 해당 작업을 실행하기 적합한 작업 서버를 고름
    + 작업 서버에게 작업 실행 지시
    + 해당 작업이 어떤 서버에 할당되었는지에 대한 정보를 실행 큐에 넣음
    + 작업이 완료되면 해당 작업을 실행 큐에서 제거

#### 작업 서버
- DAG에 정의된 작업을 수행

#### 임시 저장소
- 여러 저장소를 사용할 수 있는데 어떤 시스템을 선택할 것이냐에 따라 다름
- 예로 메타데이터는 작업 서버가 빈번히 참조하는 정보이고 크기도 작아 매모리에 캐시해두면 좋을 것. 비디오/오디오 데이터는 BLOB 저장소에 두는 것이 바람직

#### 인코딩된 비디오
- 인코딩 파이프라인의 최종 결과물

### 시스템 최적화
- 이제 속도, 안정성, 비용 측면에서 시스템을 최적화

#### 속도 최적화: 비디오 병렬 업로드
- 비디오 하나를 작은 GOP들로 분할해서 병렬적으로 업로드하면 실패해도 빠르게 업로드를 재개할 수 있음

#### 속도 최적화: 업로드 센터를 사용자 근거리에 지정
- 업로드 센터를 여러 곳에 두어서 속도 개선. 이를 위해 CDN을 업로드 센터로 이용

#### 속도 최적화: 모든 절차를 병렬화
- 느슨하게 결합된 시스템을 만들어서 병렬성을 높일 수 있음
    + 지금 설계안처럼 순서대로 진행되며 작업간 의존성이 있으면 병렬성을 높이기 어려움
- 결합도를 낮추는 예
    + 메시지 큐를 도입하면 인코딩 모듈이 다운로드 모듈의 작업이 끝나기를 기다릴 필요가 없고, 메시지 큐에 보관된 이벤트 각각을 인코딩 모듈은 병렬적으로 수행 가능

#### 안정성 최적화: 미리 사인된 업로드 URL
- 허가 받은 사용자만이 올바른 장소에 비디오를 업로드 할 수 있도록 하기 위해 pre-signed 업로드 URL을 이용
- 이를 위해 업로드 절차를 아래와 같이 변경
    1. 클라이언트가 HTTP 서버에서 POST 요청으로 pre-signed URL을 받음, 이미 접근 권한이 주어져 있는 상태임
    2. API 서버는 pre-signed URL을 돌려줌
    3. 클라이언트는 해당 URL이 가리키는 위치에 비디오를 업로드 함

#### 안전성 최적화: 비디오 보호
- 비디오 저작권을 보호하기 위해 다음 세가지 선택지 중 하나 채택
    + 디지털 저작권 관리(DRM: Digital Rights Management) 시스템 도입: 많이 사용되는 시스템은 애플의 페어플레이, 구글의 와이드바인, 마소의 플레이레디가 있음
    + AES 암호화: 비디오를 암호화하고 접근 권한을 설정하는 방식
    + 워터마크: 비디오 위에 소유자 정보를 포함하는 이미지 오버레이를 올리는 방식

#### 비용 최적화
- CDN은 비쌈. 데이터 크기가 크면 클수록 더함. 비용을 낮추려면 어떻게 해야할까
    + 인기 비디오는 CDN을 통해 재생하고 다른 비디오는 비디오 서버를 통해 재생
    + 인기가 없는 비디오는 인코딩 할 필요가 없을 수도 있음. 짧은 비디오라면 필요할 때 인코딩 해서 재생
    + 특정 비디오는 특정 지역에서만 인기가 높음. 이런 비디오는 다른 지역에 옮길 필요가 없음
    + CDN을 직접 구축하고 ISP와 제휴

### 오류 처리
- 장애를 잘 감내(fault-tolerant)하는 시스템을 만들려면 오류를 우아하게 처리하고 빠르게 회복해야 함
- 시스템 오류의 두 가지
    + 회복 가능 오류: 특정 비디오 세그먼트를 트랜스코딩하다 실패했다든가 하는 오류. 재시도 하면 해결되는 오류
    + 회복 불가능 오류: 비디오 포맷이 잘못되었거나 하는 오류. 회복 불가능한 오류가 발견되면 해당 비디오에 대한 작업을 중단하고 클라이언트에게 적절한 오류 코드를 반환
- 시스템 컴포넌트 각각에 발생할 수 있는 오류에 대한 전형적 해결 방법들
    + 업로드 오류: 몇 회 재시도
    + 비디오 분할 오류: 클라이언트가 GOP 경계에 따라 비디오를 분할하지 못하면, 전체 비디오를 서버로 전송하고 서버가 해당 비디오를 분할
    + 트랜스코딩 오류: 재시도
    + 전처리 오류: DAG 그래프 재생성
    + DAG 스케줄러 오류: 작업을 다시 스케줄링 함
    + 자원 관리자 큐에 장애 발생: 사본을 이용
    + 작업 서버 장애: 다른 서버에서 해당 작업 재시도
    + API 서버 장애: API 서버는 무상태 서버이므로 신규 요청은 다른 API 서버로 우회
    + 메타데이터 캐시 서버 장애: 데이터는 다중화되어 있으므로 다른 노드에서 데이터를 가져옴
    + 메타데이터 데이터베이스 서버 장애
        - 주 서버가 죽었다면 부 서버 가운데 하나를 주 서버로 교체
        - 부 서버가 죽었다면 다른 부 서버로 읽기 연산 처리하고 죽은 서버는 교체

# 15장 - 구글 드라이브 설계
- 구글 드라이브는 파일 저장 및 동기화 서비스로 문서, 사진, 비디오, 기타 파일을 클라우드에 보관할 수 있도록 함
- 이 파일은 컴퓨터, 스마트폰, 태블릿 등 어떤 단말에서도 이용 가능 해야 함
- 아울러 보관된 파일은 손쉽게 공유가 가능해야 함

## 1단계 - 문제 이해 및 설계 범위 확정
- 가장 중요하게 지원해야 할 기능
    + 파일 업로드/다운로드, 파일 동기화, 알림
- 모바일 앱, 웹 둘다 지원
- 파일 암호화 수행
- 파일 크기는 10GB 제한
- DAU는 천만명
- 이번 장에서 아래 기능의 설계에 집중
    + 파일 추가
    + 파일 다운로드
    + 파일 동기화
    + 파일 갱신 이력 조회
    + 파일 공유
    + 파일이 편집되거나 삭제되거나 새롭게 공유되었을 때 알림 표시
- 비-기능적 요구사항 이해
    + 안정성: 저장소 시스템에서 안정성은 아주 중요
    + 빠른 동기화 속도: 파일 동기화에 시간이 너무 많이 걸리면 안됨
    + 네트워크 대역폭: 제품이 네트워크 대역폭을 불필요하게 많이 소모한다면 사용자가 좋아하지는 않을거임. 모바일 데이터 플랜인 경우 데이터를 아껴야 됨
    + 규모 확장성: 시스템은 아주 많은 양의 트래픽도 처리 가능해야 한다
    + 높은 가용성: 일부 서버에 장애가 발생하거나, 느려지거나, 네트워크 일부가 끊겨도 시스템은 계속 사용 가능해야 함

### 개략적 추정치
- 가입 사용자는 오천만명, DAU는 천만명
- 모든 사용자에게 10GB의 무료 저장공간 할당
- 매일 각 사용자가 평균 2개의 파일을 업로드. 파일 평균 크기는 500KB
- 읽기:쓰기 비율은 1:1
- 필요한 저장공간 총량=5000천만 사용자x10GB=500페타바이트
- 업로드 API QPS=1천만 사용자 x 2회 업로드 / 24시간 / 3600초 = 약 240
- 최대 QPS = QPS x 2 = 480

## 2단계 - 개략적 설계안 제시 및 동의 구하기
- 모든 것을 담은 한 대 서버에서 출발해 점진적으로 천만 사용자 지원이 가능한 시스템으로 발전시켜 나가보기
- 우선 아래와 같은 구성의 서버 한 대로 시작
    + 파일을 올리고 다운로드 하는 과정을 처리할 웹 서버
    + 데이터 베이스
    + 파일을 저장할 저장소 시스템. 파일 저장을 위해 1TB의 공간을 사용

### API
- 기본적으로 세 가지 API가 필요
    + 파일 업로드, 다운로드, 파일 갱신 히스토리 제공

#### 파일 업로드 API
- 두 가지 종류의 업로드 지원
    + 단순 업로드: 파일 크기가 작을 때 사용
    + 이어 올리기(resumable upload): 파일 사이즈가 크고 네트워크 문제로 업로드가 중단될 가능성이 높다고 생각되면 사용
- 이어 올리기는 아래 세 단계 절차로 이루어 짐
    + 이어 올리기 URL을 받기 위한 최초 요청 전송
    + 데이터를 업로드하고 업로드 상태 모니터링
    + 업로드에 장애가 발생하면 장애 발생시점부터 업로드를 재시작

#### 파일 다운로드 API
- 예) https://api.example.com/files/download

#### 파일 갱신 히스토리 API
- 예) https://api.example.com/files/list_revisions

### 한 대 서버의 제약 극복
- 파일 시스템이 가득차면 가장 먼저 할 수 있는 해결책은 데이터를 샤딩하여 여러 서버에 나누어 저장
- 하지만 서버가 불안정해서 불안함. 이때 AWS S3는 좋은 해결책이 됨
- 아래와 같은 부분을 좀 더 연구
    + 로드밸런서: 네트워크 트래픽을 분산하기 위해 로드밸런서를 사용. 로드밸런서는 트래픽을 고르게 분산할 수 있을 뿐 아니라, 특정 웹 서버에 장애가 발생하면 자동으로 해당 서버를 우회해줌
    + 웹 서버: 로드밸런서를 추가하고 나면 더 많은 웹 서버를 손쉽게 추가 할 수 있음
    + 메타데이터 데이터베이스: 데이터베이스를 파일 저장 서버에서 분리하여 SPOF를 회피. 아울러 다중화 및 샤딩 정책을 적용하여 가용성과 규모 확장성 요구사항에 대응
    + 파일 저장소: S3를 파일 저장소로 사용하고 가용성과 데이터 무손실을 보장하기 위해 두 개 이상의 지역에 데이터를 다중화 함

### 동기화 충돌